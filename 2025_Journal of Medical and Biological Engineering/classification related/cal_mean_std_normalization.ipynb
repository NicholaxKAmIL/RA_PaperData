{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68e5fadf-198b-4ef7-9550-7eca89ebffb8",
   "metadata": {},
   "source": [
    "Calculate mean and std for normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8378746-d71d-4d84-a52c-197a5d39602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "087ae862-668b-4710-bf34-1c9f5983792c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('./hand_triple/')\n",
    "\n",
    "path = './hand_triple/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0e45872-2a91-4e7a-92f0-f7dc7687d420",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgSize = 256\n",
    "transformer = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees = 20),\n",
    "    \n",
    "    transforms.RandomHorizontalFlip(p = 0.3),\n",
    "    #transforms.RandomVerticalFlip(p = 0.3),\n",
    "    \n",
    "    transforms.Resize(size = (imgSize, imgSize), antialias = True),\n",
    "    transforms.CenterCrop(size = (imgSize, imgSize)),\n",
    "    \n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47068fca-b7a0-4d0d-a80e-ee34a30b3d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.ImageFolder(root=path, transform=transformer)\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset, batch_size = 16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d3a6e25-9675-4c1d-aba1-212c0e2bf104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_and_std(loader):\n",
    "    mean = std = 0\n",
    "    total_images_count = 0\n",
    "    for images, lables in loader:\n",
    "        batch = images.size(0)\n",
    "        images = images.view(batch, images.size(1),-1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "        total_images_count += batch\n",
    "\n",
    "    mean /= total_images_count\n",
    "    std /= total_images_count\n",
    "    return mean,std\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be1f89a2-24bd-4aa8-94ae-c134d044a756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5660, 0.5660, 0.5660]), tensor([0.2054, 0.2054, 0.2054]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mean_and_std(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c738fc1-6183-4968-b779-a6e567ef1c19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
